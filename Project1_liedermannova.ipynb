{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvnWz/LLxWYx/46o4UJmrO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kamila-Liedermannova/storybooks/blob/main/Project1_liedermannova.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "vQdXhlK6YuuK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa287580-9bb4-43fb-e415-4148f28a6f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: jamdict in /usr/local/lib/python3.10/dist-packages (0.1a11.post2)\n",
            "Requirement already satisfied: chirptext<=0.2,>=0.1 in /usr/local/lib/python3.10/dist-packages (from jamdict) (0.1.2)\n",
            "Requirement already satisfied: puchikarui<0.3,>=0.1 in /usr/local/lib/python3.10/dist-packages (from jamdict) (0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install jamdict\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CXOGhm1dLiK",
        "outputId": "862b893c-7bc4-4188-8de6-6cbfc5ff2da7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: puchikarui in /usr/local/lib/python3.10/dist-packages (0.1)\n",
            "Requirement already satisfied: jamdict in /usr/local/lib/python3.10/dist-packages (0.1a11.post2)\n",
            "Requirement already satisfied: jamdict-data in /usr/local/lib/python3.10/dist-packages (1.5)\n",
            "Requirement already satisfied: chirptext<=0.2,>=0.1 in /usr/local/lib/python3.10/dist-packages (from jamdict) (0.1.2)\n",
            "Requirement already satisfied: puchikarui<0.3,>=0.1 in /usr/local/lib/python3.10/dist-packages (from jamdict) (0.1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# use the package jamdict\n",
        "\n",
        "\n",
        "%pip install puchikarui\n",
        "%pip install --upgrade jamdict jamdict-data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jamdict import Jamdict\n",
        "jam = Jamdict()"
      ],
      "metadata": {
        "id": "qE4G_1kvNhX8"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#implementing the function for kanji characters\n",
        "def get_kanji_stroke_count(kanji_character):\n",
        "    return None"
      ],
      "metadata": {
        "id": "OYcrixY8PCxH"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of the TSV file\n",
        "url = \"https://bond-lab.github.io/Language-and-the-Computer/names.tsv\"\n"
      ],
      "metadata": {
        "id": "eF_lZVBNAfXT"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the TSV file into a DataFrame\n",
        "df = pd.read_csv(url, sep='\\t')\n"
      ],
      "metadata": {
        "id": "7lMDl_YHY-G9"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the DataFrame\n",
        "print(df.head)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wu2xJXCdZC4A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94a9a7d2-907a-41ef-d6b6-8c6c8cfa305e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of         boy   1   1.1     翔太  4639\n",
            "0       boy   1     2     拓也  4381\n",
            "1       boy   1     3     健太  4095\n",
            "2       boy   1     4      翔  2986\n",
            "3       boy   1     5     翔平  2919\n",
            "4       boy   1     6     大樹  2714\n",
            "...     ...  ..   ...    ...   ...\n",
            "62994  name  21   996  内田 大翔     4\n",
            "62995  name  21   997  内田 悠斗     4\n",
            "62996  name  21   998   内田 遥     4\n",
            "62997  name  21   999  武田 七海     4\n",
            "62998  name  21  1000  福田 蒼空     4\n",
            "\n",
            "[62999 rows x 5 columns]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the average stroke count\n",
        "def calculate_average_stroke_count(stroke_counts):\n",
        "    if len(stroke_counts) == 0:\n",
        "        return 0\n",
        "    return sum(stroke_counts) / len(stroke_counts)"
      ],
      "metadata": {
        "id": "CYyRi6E-hUP7"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define function calculate stroke counts for names in each category;\n",
        "def calculate_stroke_counts_by_category(category_data):\n",
        "    stroke_counts = []\n",
        "    for name in category_data:\n",
        "        if name.strip() != \"\":\n",
        "            if any(char >= '\\u4e00' and char <= '\\u9fff' for char in name):  # Check if the character is kanji\n",
        "                stroke_count = get_kanji_stroke_count(name)\n",
        "                if stroke_count is not None:\n",
        "                    stroke_counts.append(stroke_count)\n",
        "            else:\n",
        "                stroke_counts.append(len(name))\n",
        "    return stroke_counts\n",
        "\n",
        "#define three categories of names: boys, girls, and all names\n",
        "boys_names = df[df.iloc[:, 0] == 'boy'].iloc[:, 3].tolist()\n",
        "girls_names = df[df.iloc[:, 0] == 'girl'].iloc[:, 3].tolist()\n",
        "full_names = df.iloc[:, 3].tolist()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H_rvrx3CaJB_"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate stroke counts for all three categories\n",
        "boys_strokes_count = calculate_stroke_counts_by_category(boys_names)\n",
        "girls_strokes_count = calculate_stroke_counts_by_category(girls_names)\n",
        "full_names_strokes_count = calculate_stroke_counts_by_category(full_names)\n",
        "\n",
        "# Function to calculate the average stroke count\n",
        "def calculate_average_stroke_count(stroke_counts):\n",
        "    if len(stroke_counts) == 0:\n",
        "        return 0\n",
        "    return sum(stroke_counts) / len(stroke_counts)\n",
        "\n",
        "# Calculate average stroke counts for all three categories\n",
        "avg_boys_strokes = calculate_average_stroke_count(boys_strokes_count)\n",
        "avg_girls_strokes = calculate_average_stroke_count(girls_strokes_count)\n",
        "avg_full_names_strokes = calculate_average_stroke_count(full_names_strokes_count)\n",
        "\n",
        "# Print average stroke counts\n",
        "print(\"\\nAverage stroke count for boys' names:\", avg_boys_strokes)\n",
        "print(\"Average stroke count for girls' names:\", avg_girls_strokes)\n",
        "print(\"Average stroke count for full names:\", avg_full_names_strokes)\n",
        "\n",
        "# count if boys' names or girls' names have more strokes\n",
        "more_complicated_category = \"boys' names\" if avg_boys_strokes > avg_girls_strokes else \"girls' names\"\n",
        "print(\"Boys' names or girls' more complicated:\", more_complicated_category)\n",
        "\n",
        "# is the difference in stroke counts between boys' names and girls' names significant?\n",
        "difference_category = avg_boys_strokes - avg_girls_strokes\n",
        "if abs(difference_category) >= 1:  # Adjust threshold as needed\n",
        "    print(\"difference between boys' names and girls'significant.\")\n",
        "else:\n",
        "    print(\"difference in between boys' names and girls' not significant.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA2pEM9vabPI",
        "outputId": "1384a390-25fe-43e2-d71c-87b8113bc0a8"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average stroke count for boys' names: 3.3333333333333335\n",
            "Average stroke count for girls' names: 2.7234726688102895\n",
            "Average stroke count for full names: 2.7559614408929476\n",
            "Boys' names or girls' more complicated: boys' names\n",
            "difference in between boys' names and girls' not significant.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#boys' names separately\n",
        "boys_names = df[df.iloc[:, 0] == 'boy'].iloc[:, 3].tolist()\n",
        "\n",
        "#girls' names separately\n",
        "girls_names = df[df.iloc[:, 0] == 'girl'].iloc[:, 3].tolist()\n",
        "\n",
        "# boys' and girls' names together\n",
        "first_names = boys_names + girls_names\n",
        "\n",
        "# Calculate stroke counts for first names\n",
        "first_names_strokes_count = calculate_stroke_counts_by_category(first_names)\n",
        "\n",
        "# Calculate average stroke count for first names\n",
        "avg_first_names_strokes = calculate_average_stroke_count(first_names_strokes_count)\n",
        "\n",
        "# print average stroke count for the first names\n",
        "print(\"\\nAverage stroke count for first names (boys and girls together):\", avg_first_names_strokes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KePcf9bUbxce",
        "outputId": "b43fc0a9-a5ed-4944-faac-4f713aa04faa"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average stroke count for first names (boys and girls together): 2.7559614408929476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# names\n",
        "names = df[df.iloc[:, 0] == 'name'].iloc[:, 3].tolist()\n",
        "\n",
        "# Calculate stroke counts for names\n",
        "names_strokes_count = []\n",
        "for name in names:\n",
        "    if name.strip() != \"\":\n",
        "        # Split the name into first name and surname\n",
        "        parts = name.split()\n",
        "        for part in parts:\n",
        "            if part in first_names:\n",
        "                continue  # Skip first names as they were already counted\n",
        "            if any(char >= '\\u4e00' and char <= '\\u9fff' for char in part):\n",
        "                stroke_count = get_kanji_stroke_count(part)\n",
        "                if stroke_count is not None:\n",
        "                    names_strokes_count.append(stroke_count)\n",
        "            else:\n",
        "                names_strokes_count.append(len(part))\n",
        "\n",
        "# Calculate average stroke count for last names\n",
        "avg_last_names_strokes = calculate_average_stroke_count(names_strokes_count)\n",
        "\n",
        "# Output average stroke count for last names\n",
        "print(\"\\nAverage stroke count for last names:\", avg_last_names_strokes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0PP7YS1bB-Y",
        "outputId": "b1253f12-69c2-44ec-9747-efb8ebd38f9f"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average stroke count for last names: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract names for the \"name\" category\n",
        "names = df[df.iloc[:, 0] == 'name'].iloc[:, 3].tolist()\n",
        "\n",
        "# Calculate stroke counts for the \"name\" category\n",
        "names_strokes_count = []\n",
        "for name in names:\n",
        "    if name.strip() != \"\":\n",
        "        # Split the name into first name and surname\n",
        "        parts = name.split()\n",
        "        for part in parts:\n",
        "            if any(char >= '\\u4e00' and char <= '\\u9fff' for char in part):\n",
        "                stroke_count = get_kanji_stroke_count(part)\n",
        "                if stroke_count is not None:\n",
        "                    names_strokes_count.append(stroke_count)\n",
        "            else:\n",
        "                names_strokes_count.append(len(part))\n",
        "\n",
        "# Calculate average stroke count for the \"name\" category\n",
        "avg_names_strokes = calculate_average_stroke_count(names_strokes_count)\n",
        "\n",
        "# Output average stroke count for the \"name\" category\n",
        "print(\"\\nAverage stroke count for the 'name' category:\", avg_names_strokes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f3dd3e-483d-462f-dc30-93596a4b6232",
        "id": "IeNmgHxWlzFh"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average stroke count for the 'name' category: 2.960919540229885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Differences between years\n",
        "\n",
        "# average stroke count for each year\n",
        "average_stroke_counts_yearly = {}\n",
        "\n",
        "# each year\n",
        "for year, stroke_counts in yearly_strokes_counts.items():\n",
        "    #average stroke count for the year\n",
        "    average_stroke_count_year = sum(stroke_counts) / len(stroke_counts)\n",
        "\n",
        "    # average stroke count yearly\n",
        "    average_stroke_counts_yearly[year] = average_stroke_count_year\n",
        "\n",
        "# print average stroke counts for each year\n",
        "print(\"\\nAverage Stroke Counts for Each Year:\")\n",
        "for year, avg_stroke_count in average_stroke_counts_yearly.items():\n",
        "    print(f\"Year: {year} - Average Stroke Count: {avg_stroke_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz8gLEZmi62J",
        "outputId": "bf945ff5-3bab-414a-c288-588ebec872a5"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Stroke Counts for Each Year:\n",
            "Year: 1 - Average Stroke Count: 206.21873957985994\n",
            "Year: 2 - Average Stroke Count: 186.82\n",
            "Year: 3 - Average Stroke Count: 138.91233333333332\n",
            "Year: 4 - Average Stroke Count: 110.189\n",
            "Year: 5 - Average Stroke Count: 142.20966666666666\n",
            "Year: 6 - Average Stroke Count: 146.03766666666667\n",
            "Year: 7 - Average Stroke Count: 136.78933333333333\n",
            "Year: 8 - Average Stroke Count: 124.181\n",
            "Year: 9 - Average Stroke Count: 226.87733333333333\n",
            "Year: 10 - Average Stroke Count: 229.46533333333332\n",
            "Year: 11 - Average Stroke Count: 253.66366666666667\n",
            "Year: 12 - Average Stroke Count: 236.98466666666667\n",
            "Year: 13 - Average Stroke Count: 169.19366666666667\n",
            "Year: 14 - Average Stroke Count: 163.19333333333333\n",
            "Year: 15 - Average Stroke Count: 143.70333333333335\n",
            "Year: 16 - Average Stroke Count: 82.167\n",
            "Year: 17 - Average Stroke Count: 71.20966666666666\n",
            "Year: 18 - Average Stroke Count: 66.74433333333333\n",
            "Year: 19 - Average Stroke Count: 54.425\n",
            "Year: 20 - Average Stroke Count: 48.291666666666664\n",
            "Year: 21 - Average Stroke Count: 42.29066666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "import numpy as np\n",
        "\n",
        "# Stroke counts for each year\n",
        "stroke_counts_year = {\n",
        "    1: [206.22],\n",
        "    2: [186.82],\n",
        "    3: [138.91],\n",
        "    4: [110.19],\n",
        "    5: [142.21],\n",
        "    6: [146.04],\n",
        "    7: [136.79],\n",
        "    8: [124.18],\n",
        "    9: [226.88],\n",
        "    10: [229.47],\n",
        "    11: [253.66],\n",
        "    12: [236.98],\n",
        "    13: [169.19],\n",
        "    14: [163.19],\n",
        "    15: [143.70],\n",
        "    16: [82.17],\n",
        "    17: [71.21],\n",
        "    18: [66.74],\n",
        "    19: [54.43],\n",
        "    20: [48.29],\n",
        "    21: [42.29]\n",
        "}\n",
        "\n",
        "# Convert stroke counts to arrays\n",
        "stroke_counts_arrays = [np.array(counts) for counts in stroke_counts_year.values()]\n",
        "\n",
        "# Perform ANOVA test\n",
        "f_statistic, p_value = f_oneway(*stroke_counts_arrays)\n",
        "\n",
        "# Check if the p-value is less than the significance level (e.g., 0.05)\n",
        "significance_level = 0.05\n",
        "if p_value < significance_level:\n",
        "    print(\"significant difference between different years.\")\n",
        "else:\n",
        "    print(\"no significant difference between different years.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEryVr1ZpACe",
        "outputId": "f910a5cc-8db8-4526-9eda-4259d211ab57"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no significant difference between different years.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:4141: DegenerateDataWarning: all input arrays have length 1.  f_oneway requires that at least one input has length greater than 1.\n",
            "  warnings.warn(stats.DegenerateDataWarning(msg))\n"
          ]
        }
      ]
    }
  ]
}